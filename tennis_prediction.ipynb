{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import where\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from pandas.plotting import scatter_matrix\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode,  plot\n",
    "from plotly.graph_objs import *\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score , cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and check columns\n",
    "url = 'https://raw.githubusercontent.com/sartasos/ML-prediction-of-tennis-match-result/main/stats.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with stats that we cannot know before the match (eg \"score\", \"minutes\", \"l_1stWon\" etc)\n",
    "# also remove columns that we do not need (eg \"winner_name\",\"loser_name\" etc)\n",
    "df = df.drop(columns=[\"score\", \"tourney_name\", \"minutes\", \"l_1stIn\", \"l_1stWon\", \"l_2ndWon\", \"l_ace\", \"l_svpt\",\n",
    "                      \"l_SvGms\", \"l_bpFaced\", \"l_df\", \"l_bpSaved\", \"w_1stIn\", \"w_1stWon\", \"w_2ndWon\", \"w_ace\",\n",
    "                      \"w_svpt\", \"w_SvGms\", \"w_bpFaced\", \"w_df\", \"w_bpSaved\", \"winner_name\", \"loser_name\",\n",
    "                      \"winner_entry\", \"loser_entry\", \"tourney_id\", \"match_num\", \"winner_id\", \"winner_seed\", \"loser_id\",\n",
    "                      \"loser_seed\", \"winner_rank\", \"loser_rank\"])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values for every column\n",
    "zero_percent = df.isnull().sum() * 100 / len(df)\n",
    "zero_values_df = pd.DataFrame({\"Feature Name\": df.columns, \"Zero values count\": df.isnull().sum(),\n",
    "                               \"Zero values percent\": zero_percent})\n",
    "print(zero_values_df.reset_index().drop(columns=[\"index\"]))\n",
    "\n",
    "#there are not many columns with null values, so no action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank points and surface are considered as important features, \n",
    "# therefore remove entries that do not contain info about rank points and surface\n",
    "df.dropna(subset=[\"winner_rank_points\", \"loser_rank_points\", \"surface\"], inplace=True)\n",
    "df = df.reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert \"tourney_date\" column to new columns of \"year\" and \"month\"\n",
    "df[\"tourney-year\"] = df.tourney_date.astype(str).str[:4].astype(int)        #year column\n",
    "df[\"tourney-month\"] = df.tourney_date.astype(str).str[4:6].astype(int)      #month column\n",
    "df = df.drop(columns=[\"tourney_date\"])                                      #remove old tourney_date column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "transform our data so that we have 2 players (first & second),their personal information (hand, age, etc)\n",
    "and general information about the match and the tourney. then create 2nd copy with inverse positions\n",
    "finally create a column \"label\" in each copy which is equal to 0 if first player wins, or 1 if second player wins.\n",
    "'''\n",
    "\n",
    "df = df.rename(columns={\"loser_age\": \"first_age\", \"loser_hand\": \"first_hand\", \"loser_ht\": \"first_ht\", \"loser_ioc\": \"first_ioc\",\n",
    "                        \"loser_rank_points\": \"first_rank_points\",\n",
    "                        \"winner_age\": \"second_age\",  \"winner_hand\": \"second_hand\", \"winner_ht\": \"second_ht\",\n",
    "                        \"winner_ioc\": \"second_ioc\", \"winner_rank_points\": \"second_rank_points\"\n",
    "                        })\n",
    "\n",
    "copy_2_df = df.copy()\n",
    "copy_2_df[['first_age','first_hand','first_ht','first_ioc','first_rank_points',\n",
    "            'second_age','second_hand','second_ht','second_ioc','second_rank_points']]\\\n",
    "=copy_2_df[['second_age','second_hand','second_ht','second_ioc','second_rank_points',\n",
    "             'first_age','first_hand','first_ht','first_ioc','first_rank_points']]\n",
    "\n",
    "winner_player1 = np.zeros(copy_2_df.shape[0])  # if 1st player wins then label = 0\n",
    "copy_2_df['label'] = winner_player1\n",
    "\n",
    "winner_player2 = np.ones(df.shape[0])  #if 2nd player wins then label = 1\n",
    "df['label'] = winner_player2\n",
    "\n",
    "df = pd.concat([df,copy_2_df])\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True) #shuffle data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# height columns have some null values, so we will fill those values with the columns' mean\n",
    "df['second_ht'] = df['second_ht'].fillna(df['second_ht'].mean())\n",
    "df['first_ht'] = df['first_ht'].fillna(df['first_ht'].mean())\n",
    "\n",
    "#remove missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show correlations\n",
    "\n",
    "correlations = df.corr()\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas.get_dummies to turn columns with categorical values to numerical\n",
    "dataset1 = pd.get_dummies(df['second_hand'] , drop_first=True)\n",
    "dataset2 = pd.get_dummies(df['first_hand'], drop_first=True)\n",
    "dataset3 = pd.get_dummies(df['second_ioc'], drop_first=True)\n",
    "dataset4 = pd.get_dummies(df['first_ioc'], drop_first=True)\n",
    "dataset5 = pd.get_dummies(df['surface'], drop_first=True)\n",
    "dataset6 = pd.get_dummies(df['tourney_level'], drop_first=True)\n",
    "dataset7 = pd.get_dummies(df['round'], drop_first=True)\n",
    "\n",
    "#merge the two datasets\n",
    "merged = pd.concat([df,dataset1,dataset2,dataset3,dataset4,dataset5,dataset6,dataset7],axis='columns' )\n",
    "\n",
    "#remove old columns with categorical values that we replaced\n",
    "df = merged.drop(['second_hand','first_hand','second_ioc','first_ioc','surface','tourney_level','round'], axis='columns')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how labels are distributed in our dataset\n",
    "class_count_01, class_count_02 = df['label'].value_counts()\n",
    "df['label'].value_counts().plot(kind='bar', title='count (target)')\n",
    "print('class 0:', class_count_02)\n",
    "print('class 1:', class_count_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HISTOGRAMS \n",
    "\n",
    "hist_data = [df['second_rank_points']]\n",
    "group_labels = ['second_rank_points'] # name of the dataset\n",
    "fig = ff.create_distplot(hist_data, group_labels)\n",
    "fig.show()\n",
    "\n",
    "hist_data = [df['first_rank_points']]\n",
    "group_labels = ['first_rank_points'] # name of the dataset\n",
    "fig = ff.create_distplot(hist_data, group_labels)\n",
    "fig.show()\n",
    "\n",
    "hist_data = [df['second_ht']]\n",
    "group_labels = ['second_ht'] # name of the dataset\n",
    "fig = ff.create_distplot(hist_data, group_labels)\n",
    "fig.show()\n",
    "\n",
    "hist_data = [df['first_ht']]\n",
    "group_labels = ['first_ht'] # name of the dataset\n",
    "fig = ff.create_distplot(hist_data, group_labels)\n",
    "fig.show()\n",
    "\n",
    "hist_data = [df['second_age']]\n",
    "group_labels = ['second_age'] # name of the dataset\n",
    "fig = ff.create_distplot(hist_data, group_labels)\n",
    "fig.show()\n",
    "\n",
    "hist_data = [df['first_age']]\n",
    "group_labels = ['first_age'] # name of the dataset\n",
    "fig = ff.create_distplot(hist_data, group_labels)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split to fit and evaluate our models\n",
    "#and standardize features\n",
    "\n",
    "y = df['label'].values\n",
    "X = df.drop(['label'],axis='columns').values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)  #, random_state = 0)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Naive Bayes ####################################\n",
    "\n",
    "gaussian_classifier = GaussianNB()\n",
    "gaussian_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gaussian_classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Score:\\n\", accuracy_score)\n",
    "print(\"Training set score: {:.3f}\".format(gaussian_classifier.score(X_train,y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(gaussian_classifier.score(X_test,y_test)))\n",
    "\n",
    "report = classification_report(y_test,y_pred)  \n",
    "print(report)\n",
    "\n",
    "gaussian_classifier.class_count_\n",
    "gaussian_classifier.classes_\n",
    "gaussian_classifier.epsilon_\n",
    "gaussian_classifier.theta_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try naive bayes using kfold\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "kfold = KFold(n_splits=10) #, random_state=7)\n",
    "result1 = cross_val_score(gaussian_classifier, X, y, cv=kfold, scoring='f1_score')\n",
    "result2 = cross_val_score(gaussian_classifier, X, y, cv=kfold, scoring='recall')\n",
    "result3 = cross_val_score(gaussian_classifier, X, y, cv=kfold, scoring='precision')\n",
    "print(\"Mean F1 Score = %.2f%% - SD F1 Score = %.2f%%\" % (result1.mean()*100, result1.std()*100 ))  \n",
    "print(\"Mean Recall Score = %.2f%% - SD Recall = %.2f%%\" % (result2.mean()*100, result2.std()*100 ))\n",
    "print(\"Mean Precision Score = %.2f%% - SD Precision = %.2f%%\" % (result3.mean()*100, result3.std()*100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## SVM ######################################\n",
    "\n",
    "svm_classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Score:\\n\", accuracy_score)\n",
    "print(\"Training set score: {:.3f}\".format(svm_classifier.score(X_train,y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(svm_classifier.score(X_test,y_test)))\n",
    "\n",
    "report = classification_report(y_test,y_pred)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try SVM using kfold\n",
    "\n",
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}\n",
    "\n",
    "kfold = KFold(n_splits=10) #, random_state=7)\n",
    "result1 = cross_val_score(svm_classifier, X, y, cv=kfold, scoring='f1_score')\n",
    "result2 = cross_val_score(svm_classifier, X, y, cv=kfold, scoring='recall')\n",
    "result3 = cross_val_score(svm_classifier, X, y, cv=kfold, scoring='precision')\n",
    "print(\"Mean F1 Score = %.2f%% - SD F1 Score = %.2f%%\" % (result1.mean()*100, result1.std()*100 ))  \n",
    "print(\"Mean Recall Score = %.2f%% - SD Recall = %.2f%%\" % (result2.mean()*100, result2.std()*100 ))\n",
    "print(\"Mean Precision Score = %.2f%% - SD Precision = %.2f%%\" % (result3.mean()*100, result3.std()*100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Decision Tree #####################################\n",
    "\n",
    "dtree_classifier = DecisionTreeClassifier(criterion = 'entropy')\n",
    "dtree_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dtree_classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Score:\\n\", accuracy_score)\n",
    "print(\"Training set score: {:.3f}\".format(svm_classifier.score(X_train,y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(svm_classifier.score(X_test,y_test)))\n",
    "\n",
    "report = classification_report(y_test,y_pred)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try Decision Tree using kfold\n",
    "kfold = KFold(n_splits=10) #, random_state=7)\n",
    "result1 = cross_val_score(dtree_classifier, X, y, cv=kfold, scoring='f1_score')\n",
    "result2 = cross_val_score(dtree_classifier, X, y, cv=kfold, scoring='recall')\n",
    "result3 = cross_val_score(dtree_classifier, X, y, cv=kfold, scoring='precision')\n",
    "print(\"Mean F1 Score = %.2f%% - SD F1 Score = %.2f%%\" % (result1.mean()*100, result1.std()*100 ))  \n",
    "print(\"Mean Recall Score = %.2f%% - SD Recall = %.2f%%\" % (result2.mean()*100, result2.std()*100 ))\n",
    "print(\"Mean Precision Score = %.2f%% - SD Precision = %.2f%%\" % (result3.mean()*100, result3.std()*100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Random Forest ######################################\n",
    "\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Score:\\n\", accuracy_score)\n",
    "print(\"Training set score: {:.3f}\".format(rf_classifier.score(X_train,y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(rf_classifier.score(X_test,y_test)))\n",
    "\n",
    "#Analyze the results of Random Forest\n",
    "report = classification_report(y_test,y_pred)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest using kfold\n",
    "kfold = KFold(n_splits=10) #, random_state=7)\n",
    "result1 = cross_val_score(rf_classifier, X, y, cv=kfold, scoring='f1_score')\n",
    "result2 = cross_val_score(rf_classifier, X, y, cv=kfold, scoring='recall')\n",
    "result3 = cross_val_score(rf_classifier, X, y, cv=kfold, scoring='precision')\n",
    "print(\"Mean F1 Score = %.2f%% - SD F1 Score = %.2f%%\" % (result1.mean()*100, result1.std()*100 ))  \n",
    "print(\"Mean Recall Score = %.2f%% - SD Recall = %.2f%%\" % (result2.mean()*100, result2.std()*100 ))\n",
    "print(\"Mean Precision Score = %.2f%% - SD Precision = %.2f%%\" % (result3.mean()*100, result3.std()*100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-bolivia",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## XG Boost ######################################################\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Score:\\n\", accuracy_score)\n",
    "print(\"Training set score: {:.3f}\".format(xgb_classifier.score(X_train,y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(xgb_classifier.score(X_test,y_test)))\n",
    "\n",
    "report = classification_report(y_test,y_pred)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost using kfold\n",
    "kfold = KFold(n_splits=10) #, random_state=7)\n",
    "result1 = cross_val_score(xgb_classifier, X, y, cv=kfold, scoring='f1_score')\n",
    "result2 = cross_val_score(xgb_classifier, X, y, cv=kfold, scoring='recall')\n",
    "result3 = cross_val_score(xgb_classifier, X, y, cv=kfold, scoring='precision')\n",
    "print(\"Mean F1 Score = %.2f%% - SD F1 Score = %.2f%%\" % (result1.mean()*100, result1.std()*100 ))  \n",
    "print(\"Mean Recall Score = %.2f%% - SD Recall = %.2f%%\" % (result2.mean()*100, result2.std()*100 ))\n",
    "print(\"Mean Precision Score = %.2f%% - SD Precision = %.2f%%\" % (result3.mean()*100, result3.std()*100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## K-Nearest Neighbors ###########################################\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "knn_classifier .fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy Score:\\n\", accuracy_score)\n",
    "print(\"Training set score: {:.3f}\".format(knn_classifier.score(X_train,y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(knn_classifier.score(X_test,y_test)))\n",
    "\n",
    "report = classification_report(y_test,y_pred)  \n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN using kfold\n",
    "kfold = KFold(n_splits=10) #, random_state=7)\n",
    "result1 = cross_val_score(knn_classifier, X, y, cv=kfold, scoring='f1_score')\n",
    "result2 = cross_val_score(knn_classifier, X, y, cv=kfold, scoring='recall')\n",
    "result3 = cross_val_score(knn_classifier, X, y, cv=kfold, scoring='precision')\n",
    "print(\"Mean F1 Score = %.2f%% - SD F1 Score = %.2f%%\" % (result1.mean()*100, result1.std()*100 ))  \n",
    "print(\"Mean Recall Score = %.2f%% - SD Recall = %.2f%%\" % (result2.mean()*100, result2.std()*100 ))\n",
    "print(\"Mean Precision Score = %.2f%% - SD Precision = %.2f%%\" % (result3.mean()*100, result3.std()*100 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-resolution",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
